# Contents
1.[Introduction](# Introduction)

2.[Environment Setup](# Environment Setup)

3.[Data](# Data)

4.[How to use the code](# How to use the code)

5.[Test](# Test)

6.[Train](# Train)

7[FAQs](# FAQs)

# Introduction
These code are for peraparing data for the ideal model described in the project. In other words, these code are only used for processing the data but not training and testing the model. For testing the model and training by your own  data, you can use the data tools here and combine with the two projects in https://github.com/Fanbo0428/NIC-SWT-pytesseract and https://github.com/Fanbo0428/NIC-textboxes-pytesseract. Specifically speaking, these code are used as a demo of processing the data to implement the ideal idea. The aim of creating the training data like that is to increase the probability for NIC model to recognized whether the image contains noticeable text part in it and generate sentences come in forms of "A sth sth sth on sth with "<UNK> <UNK>" ". Here <UNK> means the text part need to be filled with. 
	
# Environment Setup
For setting up the environment, you need to make sure that you have Tensorflow (https://github.com/tensorflow) installed on your computer. In authors' computer, the version of Tensorflow is 1.8. And also tools for NLP is needed, here particularly SpaCy (https://spacy.io/). Then for generating the captions for training the model, here NIC models developed by Goolge is used (https://github.com/tensorflow/models/tree/master/research/im2txt). So you need to download the NIC model and setup the corresponding environment first. Then you can go to the directory " replace_files ", and copy the file run_inference.py to replace the orignal one in https://github.com/tensorflow/models/tree/master/research/im2txt. Remember to back up the oringinal one. 

Even though captions based on system generation are not encouraged in most of machine learning scenarios, in this case, the captions generated by NIC model will then be input into NLP systems for further operation. Since in this project, only 376 images are added, we can captioned the images by human anyway. 

For users who first use this system, some intermediate files generated by author when developing the code are attached for convenience. Also they can be benchmarks for input and output when training use your own data. 

# Data 
The data used here is from http://www.iapr-tc11.org/mediawiki/index.php/The_Street_View_Text_Dataset, called The Street View Text Dataset, besides some data created by author's own is added. The Street View Text Dataset is originally used for text recognition tasks, but due to the text here often comes in natural scenes (from Google map) and contents in pictures are not only the objects with text, so it is perfect for doing the job required in this project. 

The data used by author contains 376 images and can be downloaded on https://pan.baidu.com/s/1z1DPyk8EYtSr7JxLOm5wsQ. Also you can find data comes in TF-record form for operation simplicity. 

# How to use the code
No matter what kind of data you are using. You can use the code here to process your data for training purpose. 

rename.py: First run this code to formulize your image name for reading and writing. 

generate_captions.py: Then generate captions for each image, all the captions are stored in directory intermediate/.

find_candidates.py and find_target.py: These two files help you to locate wich words or phrases should be notice when processing the captions.

caption_processing.py: Helps you make your captions into template for training. 

DataWriter.py and DataReader.py: They are files for writing data into TF-record files and reading them out. You can also specify the scale of training and validation sets separately.  

sort_captions_template.py: Sometimes the captions may be randomly ordered after system processing them. This file will help you to sort them out following the increasing order defined by image name.

# Test
For testing the performance, you can put your trained model by the data you created yourself here into one of the projects (https://github.com/Fanbo0428/NIC-SWT-pytesseract and https://github.com/Fanbo0428/NIC-textboxes-pytesseract). And follow the instructions to see the performance. You can also see the files created each step stored in the intermediate_files/ to see how your data is processed and used. 

# Train
Training in this case have two meanings:

1. You want to train the image caption (Google NIC) model with you own data processed by this repo: see the train part in  https://github.com/tensorflow/models/tree/master/research/im2txt for details. 

2. You want to modify the SWT model for text detection in https://github.com/Fanbo0428/NIC-SWT-pytesseract for better result: Go to https://github.com/aperrau/DetectText and get the code. You can generate your own DetectText to replacr the original one for better performance. 

# FAQs

For further information and more dicussion of the idea and the code in this project, see the project report or contact me by fm2616@ic.ac.uk

  
